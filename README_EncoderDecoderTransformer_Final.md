# Encoderâ€’Decoder Transformer (MNISTÂ Grids)

<div style="display: flex; align-items: flex-start; gap: 20px;">

  <div style="flex: 1;">
    <p>
      This repository contains my <strong>week 3 project</strong> for the
      <a href="https://ml.institute/learn">Machine Learning Institute</a> six-week bootcamp.
      The challenge was to implement the full <em>Attention Is All You Need</em> architecture from scratch,
      then apply it to a fun toy task: <strong>encode multi-digit 2 &times; 2 MNIST grids and decode their sequence left-to-right</strong>.
      We followed the paperâ€™s wellâ€‘known figure and multiple blog posts (using little to no AI tooling!)
      to handâ€‘code multiâ€‘head attention, positional encodings, masking, and the encoder &amp; decoder stacks.
      It pushed us to our coding limits but came out successful! ðŸŽ‰
    </p>
  </div>

  <div style="flex: 1;">
    <img src="misc/figure.png" width="100%" alt="Transformer schematic" />
  </div>

</div>

Our team handâ€‘coded multiâ€‘head attention, positional encodings, masking, and both encoder & decoder stacks. It *hurt* â€¦ but it works! ðŸŽ‰

---

## ðŸ“¦Â Setup

> Tested on UbuntuÂ 24.04 + PythonÂ 3.10 + CUDAÂ 12.Â Replace paths if you prefer a different install location.

1. **Install MinicondaÂ +Â Mamba (1â€‘liner):**
   ```bash
   curl -sS https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh      && bash miniconda.sh -b -p "$HOME/miniconda"      && eval "$(~/miniconda/bin/conda shell.bash hook)"      && conda install -y -n base -c conda-forge mamba
   ```
2. **Create & activate the env:**
   ```bash
   mamba env create -f env.yml
   mamba activate enc_dec_env   # env name from env.yml
   ```
3. **Get the data (MNIST) & pickle it:**
   ```bash
   python misc/download_data.py
   ```
4. *(Optional)* **Verify CUDA is visible:**
   ```bash
   python misc/gpu_test.py
   ```

---

## ðŸš€Â QuickÂ Demo

1. **Download the trained checkpoint** (<3â€¯MB):  
   [GoogleÂ Drive link](https://drive.google.com/file/d/1hT5eCMmYlrDz0ZKNsQwgbBgDyISyv4LH/view?usp=sharing)

   Save it as:
   ```text
   checkpoints/enc_and_dec/final_model.pth
   ```

2. **Launch the Streamlit frontâ€‘end:**
   ```bash
   streamlit run model/frontend.py
   ```

   <p align="center">
     <img src="misc/frontend.png" width="700" alt="Streamlit demo" />
   </p>

   The app previews unseen 2â€¯Ã—â€¯2 MNIST grids from the heldâ€‘out test set, then shows the modelâ€™s predicted digit sequence.

---

## ðŸ“‹Â Dataset Info

| Split | Images | Notes |
|-------|--------|-------|
| **train** | 50â€¯000 | Standard MNIST digits (single 28â€¯Ã—â€¯28) |
| **val**   | 10â€¯000 | Heldâ€‘out MNIST digits |
| **grid**  | 60â€¯000 â†’ 15â€¯000 | Each sample is a 56â€¯Ã—â€¯56 image made by tiling 4 random digits; target = 4â€‘digit sequence |

The helper `model/image_grid_dl.py` builds the grid dataset onâ€‘theâ€‘fly and yields `(grid_img, target_seq)` pairs.

---

## ðŸ”¡Â Patch & PositionalÂ Encoding

* **`patch_and_embed.py`** slices 56â€¯Ã—â€¯56 grids into *16Â patches* (14â€¯Ã—â€¯14 each) â†’ flattens & projects to *DÂ =Â 128*.
* A learnable `[CLS]` vector is prepended so the encoder can output a single grid embedding.
* Trainable positional embeddings are added to both encoder patch tokens and decoder input tokens.

---

## ðŸ§ Â Model Overview (highâ€‘level)

| Component | File | Purpose |
|-----------|------|---------|
| **EncoderOnly** | `model/encoder_only.py` | Baseline MNIST classifier (98â€¯% val acc) |
| **Encoder** | `model/encoder.py` | 6Â layers, 8â€‘head MHâ€‘Attention, FFN, residual + LN |
| **Decoder** | `model/decoder.py` | 6Â layers with causal mask, crossâ€‘attention to encoder out |
| **Transformer** | `model/transformer.py` | Wraps encoderÂ +Â decoder â†’ predicts sequence logits |

> Loss = sum of crossâ€‘entropy over sequence positions.

---

## ðŸ‹ï¸Â Training

### 1. Encoderâ€‘only classifier (sanity check)
```bash
python model/train_enc.py
```
*Logs to WandB; hits ~98â€¯% val accuracy after 3Â epochs on RTXâ€¯4060â€‘8â€¯GB.*

### 2. Full Encoderâ€‘Decoder
```bash
python model/train_enc_dec.py
```
*Uses `model/image_grid_dl.py` (2â€¯Ã—â€¯2 grids).  
Checkpoints saved to `checkpoints/enc_and_dec/`.  
Best val seqâ€‘accuracy â‰ˆâ€¯93â€¯% in 15Â epochs.*

---

## ðŸŒÂ Inference Pipeline

```python
from model.transformer import Transformer
from PIL import Image
import torch

model = Transformer.load_from_checkpoint(
    "checkpoints/enc_and_dec/final_model.pth",
    map_location="cpu"
)
model.eval()

grid = Image.open("some_grid.png")  # 56Ã—56 PNG of 4 digits
pred = model.predict(grid)
print("Predicted seq:", pred)  # e.g. [3, 7, 1, 8]
```

---

## ðŸ“Â UsefulÂ Notebooks /Â Scripts

| Path | Description |
|------|-------------|
| `notebooks/transformer_walkthrough.ipynb` | Stepâ€‘byâ€‘step MHâ€‘Attention build *(WIP)* |
| `misc/download_data.py` | Downloads MNIST + pickles train/val splits |
| `misc/gpu_test.py` | Tiny CUDA sanity check |

---

## ðŸ“šÂ Citation

> LeCunÂ *etÂ al.* 1998. **Gradientâ€‘Based Learning Applied to Document Recognition**.  
> VaswaniÂ *etÂ al.* 2017. **Attention Is All You Need**.

---

## ðŸ—‚Â Directory Structure (trimmed)

```
.
â”œâ”€â”€ checkpoints/                # Saved models (gitâ€‘ignored)
â”‚   â””â”€â”€ enc_and_dec/
â”‚       â””â”€â”€ final_model.pth
â”œâ”€â”€ data/                       # MNIST raw & pickles (gitâ€‘ignored)
â”‚   â””â”€â”€ MNIST/
â”œâ”€â”€ env.yml                     # Conda/Mamba environment
â”œâ”€â”€ misc/
â”‚   â”œâ”€â”€ download_data.py
â”‚   â”œâ”€â”€ figure.png              # Transformer architecture figure
â”‚   â”œâ”€â”€ frontend.png            # Streamlit screenshot
â”‚   â””â”€â”€ gpu_test.py
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ encoder_only.py
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ decoder.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â”œâ”€â”€ patch_and_embed.py
â”‚   â”œâ”€â”€ image_grid_dl.py
â”‚   â”œâ”€â”€ train_enc.py
â”‚   â””â”€â”€ train_enc_dec.py
â””â”€â”€ README.md                   # You are here
```

---

## ðŸ’¡Â NextÂ Steps

1. Add beamâ€‘search decoding to improve sequence accuracy.
2. Try 3â€¯Ã—â€¯3 or randomâ€‘sized digit mosaics.
3. Replace learned positional encodings with rotaryÂ (RoPE) and compare.
4. Swap the bruteâ€‘force decoder with a lightweight **PerceiverÂ IO** variant.
